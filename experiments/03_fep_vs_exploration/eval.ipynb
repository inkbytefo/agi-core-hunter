{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0719aa",
   "metadata": {},
   "source": [
    "# Free Energy Principle vs Exploration - Results Analysis\n",
    "\n",
    "This notebook analyzes the results of our experiment testing whether Free Energy Principle agents with active inference show better exploration efficiency, adaptation to uncertainty, and surprise minimization compared to other approaches.\n",
    "\n",
    "## Hypothesis\n",
    "**Agents with stronger Free Energy Principle implementation (higher epistemic_weight) will show better exploration efficiency, faster adaptation to uncertainty, and more robust performance in dynamic environments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7600b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load results\n",
    "results_path = Path(\"results.json\")\n",
    "if results_path.exists():\n",
    "    with open(results_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(\"âœ… Results loaded successfully\")\n",
    "    print(f\"Found results for {len(results)} agents\")\n",
    "else:\n",
    "    print(\"âŒ No results file found. Run train.py first.\")\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c9618",
   "metadata": {},
   "source": [
    "## 1. Training Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Extract training curves\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    for agent_name, agent_results in results.items():\n",
    "        training_metrics = agent_results['training_metrics']\n",
    "        episodes = [m['episode'] for m in training_metrics]\n",
    "        rewards = [m['total_reward'] for m in training_metrics]\n",
    "        success_rates = [m['success'] for m in training_metrics]\n",
    "        \n",
    "        # Smooth curves with rolling average\n",
    "        window = 100\n",
    "        if len(rewards) >= window:\n",
    "            rewards_smooth = pd.Series(rewards).rolling(window).mean()\n",
    "            success_smooth = pd.Series(success_rates).rolling(window).mean()\n",
    "        else:\n",
    "            rewards_smooth = rewards\n",
    "            success_smooth = success_rates\n",
    "        \n",
    "        # Get agent-specific parameters\n",
    "        config = agent_results['agent_config']\n",
    "        if 'epistemic_weight' in config:\n",
    "            epistemic_weight = config['epistemic_weight']\n",
    "            label = f\"{agent_name} (Îµ={epistemic_weight})\"\n",
    "        else:\n",
    "            label = f\"{agent_name} (Baseline)\"\n",
    "        \n",
    "        # Plot training curves\n",
    "        axes[0, 0].plot(episodes, rewards_smooth, label=label, alpha=0.8)\n",
    "        axes[0, 1].plot(episodes, success_smooth, label=label, alpha=0.8)\n",
    "        \n",
    "        # Plot FEP-specific metrics if available\n",
    "        if 'variational_free_energy' in training_metrics[0]:\n",
    "            vfe_values = [m['variational_free_energy'] for m in training_metrics]\n",
    "            epistemic_values = [m.get('epistemic_value', 0) for m in training_metrics]\n",
    "            pragmatic_values = [m.get('pragmatic_value', 0) for m in training_metrics]\n",
    "            surprise_values = [m.get('surprise', 0) for m in training_metrics]\n",
    "            \n",
    "            axes[0, 2].plot(episodes, vfe_values, label=label, alpha=0.8)\n",
    "            axes[1, 0].plot(episodes, epistemic_values, label=label, alpha=0.8)\n",
    "            axes[1, 1].plot(episodes, pragmatic_values, label=label, alpha=0.8)\n",
    "            axes[1, 2].plot(episodes, surprise_values, label=label, alpha=0.8)\n",
    "    \n",
    "    # Configure subplots\n",
    "    axes[0, 0].set_title('Training Rewards')\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Total Reward')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].set_title('Success Rate')\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('Success Rate')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 2].set_title('Variational Free Energy')\n",
    "    axes[0, 2].set_xlabel('Episode')\n",
    "    axes[0, 2].set_ylabel('VFE')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].set_title('Epistemic Value (Information Gain)')\n",
    "    axes[1, 0].set_xlabel('Episode')\n",
    "    axes[1, 0].set_ylabel('Epistemic Value')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].set_title('Pragmatic Value (Goal Achievement)')\n",
    "    axes[1, 1].set_xlabel('Episode')\n",
    "    axes[1, 1].set_ylabel('Pragmatic Value')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 2].set_title('Surprise (Negative Log-Likelihood)')\n",
    "    axes[1, 2].set_xlabel('Episode')\n",
    "    axes[1, 2].set_ylabel('Surprise')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e6a39",
   "metadata": {},
   "source": [
    "## 2. FEP Test Scenarios Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214828de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Create comparison dataframe\n",
    "    comparison_data = []\n",
    "    \n",
    "    for agent_name, agent_results in results.items():\n",
    "        config = agent_results['agent_config']\n",
    "        epistemic_weight = config.get('epistemic_weight', 0.0)\n",
    "        standard_success = agent_results['standard_success_rate']\n",
    "        \n",
    "        # Add standard performance\n",
    "        comparison_data.append({\n",
    "            'Agent': agent_name,\n",
    "            'Epistemic_Weight': epistemic_weight,\n",
    "            'Scenario': 'Standard',\n",
    "            'Success_Rate': standard_success,\n",
    "            'Performance_Ratio': 1.0\n",
    "        })\n",
    "        \n",
    "        # Add FEP test performances\n",
    "        fep_results = agent_results['fep_results']\n",
    "        fep_ratios = agent_results['fep_performance_ratios']\n",
    "        \n",
    "        for key, success_rate in fep_results.items():\n",
    "            if key.endswith('_success_rate'):\n",
    "                scenario = key.replace('_success_rate', '')\n",
    "                ratio_key = f\"{scenario}_performance_ratio\"\n",
    "                ratio = fep_ratios.get(ratio_key, 0)\n",
    "                \n",
    "                comparison_data.append({\n",
    "                    'Agent': agent_name,\n",
    "                    'Epistemic_Weight': epistemic_weight,\n",
    "                    'Scenario': scenario.replace('_', ' ').title(),\n",
    "                    'Success_Rate': success_rate,\n",
    "                    'Performance_Ratio': ratio\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Plot FEP performance comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Success rates by scenario\n",
    "    sns.barplot(data=df, x='Scenario', y='Success_Rate', hue='Agent', ax=axes[0])\n",
    "    axes[0].set_title('Success Rates by FEP Test Scenario')\n",
    "    axes[0].set_ylabel('Success Rate')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Performance ratios (FEP tests vs Standard)\n",
    "    fep_df = df[df['Scenario'] != 'Standard']\n",
    "    sns.barplot(data=fep_df, x='Scenario', y='Performance_Ratio', hue='Agent', ax=axes[1])\n",
    "    axes[1].set_title('FEP Test Performance Ratios (Test/Standard)')\n",
    "    axes[1].set_ylabel('Performance Ratio')\n",
    "    axes[1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Baseline')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display summary table\n",
    "    print(\"\\nðŸ“Š Performance Summary Table:\")\n",
    "    summary_table = df.pivot(index='Agent', columns='Scenario', values='Success_Rate')\n",
    "    summary_table['Epistemic_Weight'] = df.groupby('Agent')['Epistemic_Weight'].first()\n",
    "    summary_table = summary_table.sort_values('Epistemic_Weight')\n",
    "    print(summary_table.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e5d62",
   "metadata": {},
   "source": [
    "## 3. Active Inference Metrics Analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
